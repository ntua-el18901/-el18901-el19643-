{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bee0fa-1752-4e56-9553-6a89836ccb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------+\n",
      "|division  |average_distance  |#      |\n",
      "+----------+------------------+-------+\n",
      "|HARBOR    |10896.762642401442|2476518|\n",
      "|HOLLENBECK|12571.896424760367|2579   |\n",
      "+----------+------------------+-------+\n",
      "\n",
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- average_distance: double (nullable = true)\n",
      " |-- #: long (nullable = false)\n",
      "\n",
      "Time taken: 24.62 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, count, row_number, min, sum, avg, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import time\n",
    "\n",
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 5 with 4 core/8 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read crime and police stations dataframes\n",
    "crime_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "ps_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "\n",
    "crime_df_1 = spark.read.csv(crime_path_1, header=True, inferSchema=True)\n",
    "crime_df_2 = spark.read.csv(crime_path_2, header=True, inferSchema=True)\n",
    "ps_df = spark.read.csv(ps_path, header=True, inferSchema=True)\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create crime dataframe combining 2 crime dataframes\n",
    "crime_df = crime_df_1.union(crime_df_2)\n",
    "\n",
    "# We filter crime dataframe with victims of positive age\n",
    "# we create a key attribute called crime_id\n",
    "# we calculate the crime_location using crime coordinates\n",
    "# we select the crime_id and the crime_location\n",
    "crime_df = crime_df.filter(col(\"Vict Age\") > 0) \\\n",
    "   .withColumn(\"Crime_Location\", ST_Point(\"LAT\", \"LON\")) \\\n",
    "   .withColumn(\"Crime_id\", monotonically_increasing_id()) \\\n",
    "   .select(\"Crime_id\", \"Crime_Location\")\n",
    "\n",
    "# we calculate the ps_location using the police station coordinates\n",
    "# we select the division and the ps_location\n",
    "# since the division attribute can be used as a key attribute\n",
    "ps_df = ps_df.withColumn(\"PS_Location\", ST_Point(\"X\", \"Y\")) \\\n",
    "             .select(\"DIVISION\", \"PS_Location\")\n",
    "\n",
    "# we make a new dataframe called df\n",
    "# which is the crossjoin of crime_df and ps_df\n",
    "# to find all possible combination between both dataframes\n",
    "# and we calculate the distance between each location\n",
    "df = crime_df.join(ps_df) \\\n",
    "             .withColumn(\"Distance_km\", ST_DistanceSphere(\"Crime_Location\", \"PS_Location\")/1000) \\\n",
    "             .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "# we calculate for each crime the minimum distance\n",
    "# to find the closest police station for each crime\n",
    "# and we filter the df such that\n",
    "# for each crime we keep the closest police station\n",
    "# and the distance between them\n",
    "windowSpec = Window.partitionBy(\"Crime_id\").orderBy(\"Distance_km\")\n",
    "df = df.withColumn(\"num_row\", row_number().over(windowSpec))\n",
    "\n",
    "df = df.filter(col(\"num_row\") == 1) \\\n",
    "       .select(\"Crime_id\", \"division\", \"Distance_km\")\n",
    "\n",
    "# then we broupby the division attribute\n",
    "# and calculate the count and the average distance such that\n",
    "# for each police station we keep how many crimes occured closer\n",
    "# to that police station and the average distance between them\n",
    "df = df.groupBy(\"division\") \\\n",
    "       .agg(count(\"*\").alias(\"#\"),\n",
    "            avg(\"Distance_km\").alias(\"average_distance\"))\n",
    "\n",
    "# finally we select the division, average_distance and the count '#'\n",
    "# sorted by '#'\n",
    "result_df = df.select(\"division\", \"average_distance\", \"#\").sort(\"#\", ascending=False)\n",
    "\n",
    "result_df.show(truncate=False)\n",
    "result_df.printSchema()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96b01e1-3cde-4c71-b5ac-27a474a998ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------+\n",
      "|division  |average_distance  |#      |\n",
      "+----------+------------------+-------+\n",
      "|HARBOR    |10896.762642401449|2476518|\n",
      "|HOLLENBECK|12571.896424760362|2579   |\n",
      "+----------+------------------+-------+\n",
      "\n",
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- average_distance: double (nullable = true)\n",
      " |-- #: long (nullable = false)\n",
      "\n",
      "Time taken: 17.85 seconds"
     ]
    }
   ],
   "source": [
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 5 with 2 core/4 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read crime and police stations dataframes\n",
    "crime_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "ps_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "\n",
    "crime_df_1 = spark.read.csv(crime_path_1, header=True, inferSchema=True)\n",
    "crime_df_2 = spark.read.csv(crime_path_2, header=True, inferSchema=True)\n",
    "ps_df = spark.read.csv(ps_path, header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "crime_df = crime_df_1.union(crime_df_2)\n",
    "\n",
    "crime_df = crime_df.filter(col(\"Vict Age\") > 0) \\\n",
    "   .withColumn(\"Crime_Location\", ST_Point(\"LAT\", \"LON\")) \\\n",
    "   .withColumn(\"Crime_id\", monotonically_increasing_id()) \\\n",
    "   .select(\"Crime_id\", \"Crime_Location\")\n",
    "\n",
    "ps_df = ps_df.withColumn(\"PS_Location\", ST_Point(\"X\", \"Y\")) \\\n",
    "             .select(\"DIVISION\", \"PS_Location\")\n",
    "\n",
    "df = crime_df.join(ps_df) \\\n",
    "             .withColumn(\"Distance_km\", ST_DistanceSphere(\"Crime_Location\", \"PS_Location\")/1000) \\\n",
    "             .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Crime_id\").orderBy(\"Distance_km\")\n",
    "df = df.withColumn(\"num_row\", row_number().over(windowSpec))\n",
    "\n",
    "df = df.filter(col(\"num_row\") == 1) \\\n",
    "       .select(\"Crime_id\", \"division\", \"Distance_km\")\n",
    "\n",
    "df = df.groupBy(\"division\") \\\n",
    "       .agg(count(\"*\").alias(\"#\"),\n",
    "            avg(\"Distance_km\").alias(\"average_distance\"))\n",
    "\n",
    "result_df = df.select(\"division\", \"average_distance\", \"#\").sort(\"#\", ascending=False)\n",
    "\n",
    "result_df.show(truncate=False)\n",
    "result_df.printSchema()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0a2dc3-2c58-46a7-b7d3-ff0b278c7c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------+\n",
      "|division  |average_distance  |#      |\n",
      "+----------+------------------+-------+\n",
      "|HARBOR    |10896.76264240145 |2476518|\n",
      "|HOLLENBECK|12571.896424760362|2579   |\n",
      "+----------+------------------+-------+\n",
      "\n",
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- average_distance: double (nullable = true)\n",
      " |-- #: long (nullable = false)\n",
      "\n",
      "Time taken: 6.99 seconds"
     ]
    }
   ],
   "source": [
    "# Create spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 5 with 2 core/4 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read crime and police stations dataframes\n",
    "crime_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "ps_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "\n",
    "crime_df_1 = spark.read.csv(crime_path_1, header=True, inferSchema=True)\n",
    "crime_df_2 = spark.read.csv(crime_path_2, header=True, inferSchema=True)\n",
    "ps_df = spark.read.csv(ps_path, header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "crime_df = crime_df_1.union(crime_df_2)\n",
    "\n",
    "crime_df = crime_df.filter(col(\"Vict Age\") > 0) \\\n",
    "   .withColumn(\"Crime_Location\", ST_Point(\"LAT\", \"LON\")) \\\n",
    "   .withColumn(\"Crime_id\", monotonically_increasing_id()) \\\n",
    "   .select(\"Crime_id\", \"Crime_Location\")\n",
    "\n",
    "ps_df = ps_df.withColumn(\"PS_Location\", ST_Point(\"X\", \"Y\")) \\\n",
    "             .select(\"DIVISION\", \"PS_Location\")\n",
    "\n",
    "df = crime_df.join(ps_df) \\\n",
    "             .withColumn(\"Distance_km\", ST_DistanceSphere(\"Crime_Location\", \"PS_Location\")/1000) \\\n",
    "             .withColumnRenamed(\"DIVISION\", \"division\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Crime_id\").orderBy(\"Distance_km\")\n",
    "df = df.withColumn(\"num_row\", row_number().over(windowSpec))\n",
    "\n",
    "df = df.filter(col(\"num_row\") == 1) \\\n",
    "       .select(\"Crime_id\", \"division\", \"Distance_km\")\n",
    "\n",
    "df = df.groupBy(\"division\") \\\n",
    "       .agg(count(\"*\").alias(\"#\"),\n",
    "            avg(\"Distance_km\").alias(\"average_distance\"))\n",
    "\n",
    "result_df = df.select(\"division\", \"average_distance\", \"#\").sort(\"#\", ascending=False)\n",
    "\n",
    "result_df.show(truncate=False)\n",
    "result_df.printSchema()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
