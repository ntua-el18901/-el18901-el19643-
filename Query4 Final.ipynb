{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71114ced-6819-4922-ab90-67f5b8430857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3684</td><td>application_1732639283265_3630</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3630/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3630_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|  9|\n",
      "|               Other|  3|\n",
      "|Hispanic/Latin/Me...|  2|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|Hispanic/Latin/Me...|533|\n",
      "|               Black|305|\n",
      "|               White| 48|\n",
      "|               Other| 30|\n",
      "|         Other Asian|  6|\n",
      "|             Unknown|  3|\n",
      "|              Korean|  3|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 58.09 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 with 1 core/2 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# εχουμε κρατησει απο το 3ο query τους υπολογισμους για το κατα\n",
    "# κεφαλην εισοδημα της καθε περιοχης\n",
    "file_capita = \"s3://groups-bucket-dblab-905418150721/group25/\\\n",
    "query4/Income per capita/\"\n",
    "df_income = spark.read.csv(file_capita, header=True, inferSchema=True)\n",
    "# κραταμε μονο περιοχες και κατα κεφαλην εισοδημα και ταξινομουμε\n",
    "# με βαση αυτο για να κρατησουμε στην συνεχεια τις τρεις πρωτες\n",
    "# περιοχες και τις τρεις τελευταιες σε διαφορετικα dataframes\n",
    "df_income = df_income.select(\"LOCATIONS\", \"income per capita\") \\\n",
    "                        .sort(\"income per capita\", ascending=False)\n",
    "top3 = df_income.limit(3)\n",
    "bottom3 = spark.createDataFrame(df_income.tail(3),\n",
    "                                [\"LOCATIONS\", \"income per capita\"])\n",
    "dis_loc = top3.union(bottom3)\n",
    "\n",
    "# διαβαζουμε μονο το ενα συνολο δεδομενων επειδη σε αυτο εχουμε\n",
    "# εγκληματα απο το 2015 και διαβαζουμε και το csv που περιεχει\n",
    "# τις κωδικοποιησεις των εθνηκοτητων\n",
    "file_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "file_3 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "RE_codes.csv\"\n",
    "df = spark.read.csv(file_1, header=True, inferSchema=True)\n",
    "df3 = spark.read.csv(file_3, header=True, inferSchema=True)\n",
    "# κραταμε εγκυρες περιπτωσεις, θετικες ηλικιες, οχι null εθνικοτητες,\n",
    "# και μονο συμβαντα του 2015\n",
    "df = df.filter((col(\"Vict Age\") > 0)) \\\n",
    "        .filter(col(\"Vict Descent\").isNotNull()) \\\n",
    "        .filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "# κραταμε τις στηλες που μας ενδιαφερουν\n",
    "df = df.select(\"Vict Descent\", \"LOCATION\", \"Cross Street\") \\\n",
    "        .withColumnRenamed(\"Cross Street\", \"cross_street\")\n",
    "# κανουμε join μονο για συμβαντα που εγιναν στις περιοχες που ζηταμε\n",
    "df = dis_loc.join(df, df.LOCATION.contains(dis_loc.LOCATIONS) |\n",
    "                  df.cross_street.contains(dis_loc.LOCATIONS), \"inner\")\n",
    "# κανουμε groupBy με βαση την περιοχη και την εθνικοτητα και\n",
    "# προσθετουμε τις φορες εμφανισης για καθε εθνικοτητα ξεχωριστα\n",
    "df = df.groupBy(\"LOCATIONS\", \"Vict Descent\") \\\n",
    "           .agg(count(\"Vict Descent\").alias(\"#\"))\n",
    "# κανουμε join με τις κωδικοποιησεις ωστε να παψουμε να εχουμε\n",
    "# μονο ενα γραμμα αλλα την πληρη περιγραφη της εθνικοτητας\n",
    "result_df = df.join(df3, on=\"Vict Descent\", how=\"inner\") \\\n",
    "                .select(\"LOCATIONS\", \"Vict Descent Full\", \"#\") \\\n",
    "                .withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "# τελος κανουμε join μια φορα για τις 3 πρωτες περιοχες και στη συνεχεια\n",
    "# για τις 3 τελευταιες\n",
    "result_df_fortop3 = result_df.join(top3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_top3 = result_df_fortop3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_top3.show()\n",
    "\n",
    "result_df_forbottom3 = result_df.join(bottom3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_bottom3 = result_df_forbottom3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_bottom3.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a445adc-b338-43a3-92a8-733fe6c1f085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|  9|\n",
      "|               Other|  3|\n",
      "|Hispanic/Latin/Me...|  2|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|Hispanic/Latin/Me...|533|\n",
      "|               Black|305|\n",
      "|               White| 48|\n",
      "|               Other| 30|\n",
      "|         Other Asian|  6|\n",
      "|             Unknown|  3|\n",
      "|              Korean|  3|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 26.66 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 with 2 core/4 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# εχουμε κρατησει απο το 3ο query τους υπολογισμους για το κατα\n",
    "# κεφαλην εισοδημα της καθε περιοχης\n",
    "file_capita = \"s3://groups-bucket-dblab-905418150721/group25/\\\n",
    "query4/Income per capita/\"\n",
    "df_income = spark.read.csv(file_capita, header=True, inferSchema=True)\n",
    "# κραταμε μονο περιοχες και κατα κεφαλην εισοδημα και ταξινομουμε\n",
    "# με βαση αυτο για να κρατησουμε στην συνεχεια τις τρεις πρωτες\n",
    "# περιοχες και τις τρεις τελευταιες σε διαφορετικα dataframes\n",
    "df_income = df_income.select(\"LOCATIONS\", \"income per capita\") \\\n",
    "                        .sort(\"income per capita\", ascending=False)\n",
    "top3 = df_income.limit(3)\n",
    "bottom3 = spark.createDataFrame(df_income.tail(3),\n",
    "                                [\"LOCATIONS\", \"income per capita\"])\n",
    "dis_loc = top3.union(bottom3)\n",
    "# διαβαζουμε μονο το ενα συνολο δεδομενων επειδη σε αυτο εχουμε\n",
    "# εγκληματα απο το 2015 και διαβαζουμε και το csv που περιεχει\n",
    "# τις κωδικοποιησεις των εθνηκοτητων\n",
    "file_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "file_3 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "RE_codes.csv\"\n",
    "df = spark.read.csv(file_1, header=True, inferSchema=True)\n",
    "df3 = spark.read.csv(file_3, header=True, inferSchema=True)\n",
    "# κραταμε εγκυρες περιπτωσεις, θετικες ηλικιες, οχι null εθνικοτητες,\n",
    "# και μονο συμβαντα του 2015\n",
    "df = df.filter((col(\"Vict Age\") > 0)) \\\n",
    "        .filter(col(\"Vict Descent\").isNotNull()) \\\n",
    "        .filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "# κραταμε τις στηλες που μας ενδιαφερουν\n",
    "df = df.select(\"Vict Descent\", \"LOCATION\", \"Cross Street\") \\\n",
    "        .withColumnRenamed(\"Cross Street\", \"cross_street\")\n",
    "# κανουμε join μονο για συμβαντα που εγιναν στις περιοχες που ζηταμε\n",
    "df = dis_loc.join(df, df.LOCATION.contains(dis_loc.LOCATIONS) |\n",
    "                  df.cross_street.contains(dis_loc.LOCATIONS), \"inner\")\n",
    "# κανουμε groupBy με βαση την περιοχη και την εθνικοτητα και\n",
    "# προσθετουμε τις φορες εμφανισης για καθε εθνικοτητα ξεχωριστα\n",
    "df = df.groupBy(\"LOCATIONS\", \"Vict Descent\") \\\n",
    "           .agg(count(\"Vict Descent\").alias(\"#\"))\n",
    "# κανουμε join με τις κωδικοποιησεις ωστε να παψουμε να εχουμε\n",
    "# μονο ενα γραμμα αλλα την πληρη περιγραφη της εθνικοτητας\n",
    "result_df = df.join(df3, on=\"Vict Descent\", how=\"inner\") \\\n",
    "                .select(\"LOCATIONS\", \"Vict Descent Full\", \"#\") \\\n",
    "                .withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "# τελος κανουμε join μια φορα για τις 3 πρωτες περιοχες και στη συνεχεια\n",
    "# για τις 3 τελευταιες\n",
    "result_df_fortop3 = result_df.join(top3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_top3 = result_df_fortop3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_top3.show()\n",
    "\n",
    "result_df_forbottom3 = result_df.join(bottom3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_bottom3 = result_df_forbottom3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_bottom3.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064159d3-9a05-4221-8b51-e61f1873af6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|  9|\n",
      "|               Other|  3|\n",
      "|Hispanic/Latin/Me...|  2|\n",
      "+--------------------+---+\n",
      "\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|Hispanic/Latin/Me...|533|\n",
      "|               Black|305|\n",
      "|               White| 48|\n",
      "|               Other| 30|\n",
      "|         Other Asian|  6|\n",
      "|             Unknown|  3|\n",
      "|              Korean|  3|\n",
      "|             Chinese|  1|\n",
      "|American Indian/A...|  1|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 26.02 seconds"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 with 4 core/8 Gb memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# εχουμε κρατησει απο το 3ο query τους υπολογισμους για το κατα κεφαλην\n",
    "# εισοδημα της καθε περιοχης\n",
    "file_capita = \"s3://groups-bucket-dblab-905418150721/group25/query4/\\\n",
    "Income per capita/\"\n",
    "df_income = spark.read.csv(file_capita, header=True, inferSchema=True)\n",
    "# κραταμε μονο περιοχες και κατα κεφαλην εισοδημα και ταξινομουμε\n",
    "# με βαση αυτο για να κρατησουμε στην συνεχεια τις τρεις πρωτες\n",
    "# περιοχες και τις τρεις τελευταιες σε διαφορετικα dataframes\n",
    "df_income = df_income.select(\"LOCATIONS\", \"income per capita\") \\\n",
    "                        .sort(\"income per capita\", ascending=False)\n",
    "top3 = df_income.limit(3)\n",
    "bottom3 = spark.createDataFrame(df_income.tail(3),\n",
    "                                [\"LOCATIONS\", \"income per capita\"])\n",
    "dis_loc = top3.union(bottom3)\n",
    "# διαβαζουμε μονο το ενα συνολο δεδομενων επειδη σε αυτο εχουμε\n",
    "# εγκληματα απο το 2015 και διαβαζουμε και το csv που περιεχει\n",
    "# τις κωδικοποιησεις των εθνηκοτητων\n",
    "file_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "file_3 = \"s3://initial-notebook-data-bucket-dblab-905418150721/\\\n",
    "RE_codes.csv\"\n",
    "df = spark.read.csv(file_1, header=True, inferSchema=True)\n",
    "df3 = spark.read.csv(file_3, header=True, inferSchema=True)\n",
    "# κραταμε εγκυρες περιπτωσεις, θετικες ηλικιες, οχι null εθνικοτητες,\n",
    "# και μονο συμβαντα του 2015\n",
    "df = df.filter((col(\"Vict Age\") > 0)) \\\n",
    "        .filter(col(\"Vict Descent\").isNotNull()) \\\n",
    "        .filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "# κραταμε τις στηλες που μας ενδιαφερουν\n",
    "df = df.select(\"Vict Descent\", \"LOCATION\", \"Cross Street\") \\\n",
    "        .withColumnRenamed(\"Cross Street\", \"cross_street\")\n",
    "# κανουμε join μονο για συμβαντα που εγιναν στις περιοχες που ζηταμε\n",
    "df = dis_loc.join(df, df.LOCATION.contains(dis_loc.LOCATIONS) |\n",
    "                  df.cross_street.contains(dis_loc.LOCATIONS), \"inner\")\n",
    "# κανουμε groupBy με βαση την περιοχη και την εθνικοτητα και\n",
    "# προσθετουμε τις φορες εμφανισης για καθε εθνικοτητα ξεχωριστα\n",
    "df = df.groupBy(\"LOCATIONS\", \"Vict Descent\") \\\n",
    "           .agg(count(\"Vict Descent\").alias(\"#\"))\n",
    "# κανουμε join με τις κωδικοποιησεις ωστε να παψουμε να εχουμε\n",
    "# μονο ενα γραμμα αλλα την πληρη περιγραφη της εθνικοτητας\n",
    "result_df = df.join(df3, on=\"Vict Descent\", how=\"inner\") \\\n",
    "                .select(\"LOCATIONS\", \"Vict Descent Full\", \"#\") \\\n",
    "                .withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "# τελος κανουμε join μια φορα για τις 3 πρωτες περιοχες και στη συνεχεια\n",
    "# για τις 3 τελευταιες\n",
    "result_df_fortop3 = result_df.join(top3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_top3 = result_df_fortop3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_top3.show()\n",
    "\n",
    "result_df_forbottom3 = result_df.join(bottom3, on=\"LOCATIONS\", how=\"inner\")\n",
    "result_bottom3 = result_df_forbottom3.groupBy(\"Vict Descent\") \\\n",
    "                                .agg(sum(\"#\").alias(\"#\")) \\\n",
    "                                .sort(\"#\", ascending=False) \\\n",
    "                                .withColumnRenamed(\"Vict Descent\",\n",
    "                                                   \"Victim Descent\")\n",
    "result_bottom3.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
